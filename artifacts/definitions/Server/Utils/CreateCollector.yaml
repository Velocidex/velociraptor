name: Server.Utils.CreateCollector
description: |
  A utility artifact to create a stand alone collector.

  This artifact is actually invoked by the Offline collector GUI and
  that is the recommended way to launch it. You can find the Offline
  collector builder in the `Server Artifacts` section of the GUI.

type: SERVER

tools:
  - name: VelociraptorWindows
    github_project: Velocidex/velociraptor
    github_asset_regex: windows-amd64.exe
    serve_locally: true

  - name: VelociraptorWindows_x86
    github_project: Velocidex/velociraptor
    github_asset_regex: windows-386.exe
    serve_locally: true

  - name: VelociraptorLinux
    github_project: Velocidex/velociraptor
    github_asset_regex: linux-amd64
    serve_locally: true

  - name: VelociraptorDarwin
    github_project: Velocidex/velociraptor
    github_asset_regex: darwin-amd64
    serve_locally: true

parameters:
  - name: OS
    default: Windows
    type: choices
    choices:
      - Windows
      - Windows_x86
      - Linux
      - MacOS

  - name: artifacts
    description: A list of artifacts to collect
    type: json_array
    default: |
      ["Generic.Client.Info"]

  - name: template
    default:
    description: The HTML report template to use.

  - name: Password
    description: If set we encrypt collected zip files with this password.

  - name: parameters
    description: A dict containing the parameters to set.
    type: json
    default: |
      {}

  - name: target
    description: Output type
    type: choices
    default: ZIP
    choices:
      - ZIP
      - GCS
      - S3
      - SFTP

  - name: target_args
    description: Type Dependent args
    type: json
    default: "{}"

  - name: opt_verbose
    default: Y
    type: bool
    description: Show verbose progress.

  - name: opt_banner
    default: Y
    type: bool
    description: Show Velociraptor banner.

  - name: opt_prompt
    default: N
    type: bool
    description: Wait for a prompt before closing.

  - name: opt_admin
    default: Y
    type: bool
    description: Require administrator privilege when running.

  - name: opt_tempdir
    default:
    description: A directory to write tempfiles in

  - name: opt_level
    default: "4"
    type: int
    description: Compression level (0=no compression).

  - name: opt_format
    default: "jsonl"
    description: Output format (jsonl or csv)

  - name: opt_output_directory
    default: ""
    description: An optional output directory prefix

  - name: StandardCollection
    type: hidden
    default: |
      // Add all the tools we are going to use to the inventory.
      LET _ <= SELECT inventory_add(tool=ToolName, hash=ExpectedHash)
       FROM parse_csv(filename="/inventory.csv", accessor="me")
       WHERE log(message="Adding tool " + ToolName)

      LET baseline <= SELECT Fqdn FROM info()

      // Make the filename safe on windows but we trust the OutputPrefix.
      LET filename <= OutputPrefix + regex_replace(
          source=format(format="Collection-%s-%s",
                        args=[baseline[0].Fqdn,
                              timestamp(epoch=now()).MarshalText]),
          re="[^0-9A-Za-z\\-]", replace="_")

      LET _ <= log(message="Will collect package " + filename)
      LET report_filename <= if(condition=Template, then=filename + ".html")
      SELECT * FROM collect(artifacts=Artifacts, report=report_filename,
          args=Parameters, output=filename + ".zip", template=Template,
          password=Password, level=Level, format=Format)

  - name: S3Collection
    type: hidden
    default: |
      // A utility function to upload the file.
      LET upload_file(filename, name, accessor) = upload_s3(
          file=filename,
          accessor=accessor,
          bucket=TargetArgs.bucket,
          name=name,
          credentialskey=TargetArgs.credentialsKey,
          credentialssecret=TargetArgs.credentialsSecret,
          region=TargetArgs.region,
          endpoint=TargetArgs.endpoint,
          serversideencryption=TargetArgs.serverSideEncryption,
          noverifycert=TargetArgs.noverifycert)

  - name: GCSCollection
    type: hidden
    default: |
      // A utility function to upload the file.
      LET upload_file(filename, name, accessor) = upload_gcs(
          file=filename,
          accessor=accessor,
          bucket=TargetArgs.bucket,
          project=GCSBlob.project_id,
          name=name,
          credentials=TargetArgs.GCSKey)

  - name: SFTPCollection
    type: hidden
    default : |
      LET upload_file(filename, name, accessor) = upload_sftp(
        file=filename,
        accessor=accessor,
        name=name,
        user=TargetArgs.user,
        path=TargetArgs.path,
        privatekey=TargetArgs.privatekey,
        endpoint=TargetArgs.endpoint,
        hostkey = TargetArgs.hostkey)

  - name: CloudCollection
    type: hidden
    default: |
      // Add all the tools we are going to use to the inventory.
      LET _ <= SELECT inventory_add(tool=ToolName, hash=ExpectedHash)
      FROM parse_csv(filename="/inventory.csv", accessor="me")
      WHERE log(message="Adding tool " + ToolName)

      LET baseline <= SELECT Fqdn, basename(path=Exe) AS Exe FROM info()
      LET TargetArgs <= target_args

      // Make the filename safe on windows but we trust the OutputPrefix.
      LET filename <= OutputPrefix + regex_replace(
          source=format(format="Collection-%s-%s",
                        args=[baseline[0].Fqdn,
                              timestamp(epoch=now()).MarshalText]),
          re="[^0-9A-Za-z\\-]", replace="_")

      // Try to upload the log file now to see if we are even able to
      // upload at all - we do this to avoid having to collect all the
      // data and then failing the upload step.
      LET _ <= log(message="Uploading to " + filename + ".log")
      LET upload_test <= upload_file(
          filename="Test upload from " + baseline[0].Exe,
          accessor="data",
          name=filename + ".log")

      LET _ <= log(message="Will collect package " + filename +
         " and upload to cloud bucket " + TargetArgs.bucket)
      LET report_filename <= if(condition=Template, then=tempfile(extension=".html"))
      LET collect_and_upload = SELECT
          upload_file(filename=Container,
                      name=filename+".zip",
                      accessor="file") AS Upload,
          if(condition=Template,
             then=upload_file(filename=Report,
                              name=filename+".html",
                              accessor="file")) AS ReportUpload,
          upload_file(filename=baseline[0].Exe + ".log",
                      name=filename+".log",
                      accessor="file") AS LogUpload
      FROM collect(artifacts=Artifacts,
          report=report_filename,
          args=Parameters,
          format=Format,
          output=tempfile(extension=".zip"),
          template=Template,
          password=Password,
          level=Level)

      SELECT * FROM if(condition=upload_test.Path,
          then=collect_and_upload,
          else={SELECT log(message="Aborting collection: Failed to upload to cloud bucket!")
                FROM scope()})

  - name: PackageToolsArtifact
    description: Collects and uploads third party binaries.
    type: hidden
    default: |
      name: PackageToolsArtifact
      parameters:
       - name: Binaries
         type: json_array
         default: '[]'
      sources:
       - query: |
          LET temp <= tempfile()

          LET uploader = SELECT ToolName,
                                Upload.Path AS Filename,
                                Upload.sha256 AS ExpectedHash,
                                Upload.Size AS Size
          FROM foreach(row=Binaries,
            query={
              SELECT _value AS ToolName, upload(file=FullPath, name=Name) AS Upload
              FROM Artifact.Generic.Utils.FetchBinary(
                   ToolName=_value, SleepDuration='0',
                   ToolInfo=inventory_get(tool=_value))
            })

          // Flush the entire query into the inventory file.
          LET _ <= SELECT * FROM write_csv(filename=temp, query=uploader)

          // Now upload it.
          SELECT upload(file=temp, name="inventory.csv") FROM scope()

  - name: FetchBinaryOverride
    type: hidden
    description: |
       A replacement for Generic.Utils.FetchBinary which
       grabs files from the local archive.

    default: |
       LET temp_binary <= tempfile(extension=".exe",
                remove_last=TRUE, permissions="x")

       LET matching_tools = SELECT ToolName AS ArchiveTool, Filename
       FROM parse_csv(filename="/inventory.csv", accessor="me")

       SELECT * FROM foreach(row=matching_tools, query={
         SELECT copy(filename=Filename, accessor="me", dest=temp_binary) AS FullPath,
                     Filename AS Name
         FROM scope()
         WHERE ToolName = ArchiveTool
       })

sources:
  - query: |
      LET Payload <= tempfile(extension=".zip")
      LET Binaries <= SELECT * FROM foreach(
          row={
             SELECT tools FROM artifact_definitions(names=artifacts)
          }, query={
             SELECT * FROM foreach(row=tools,
             query={
              SELECT name AS Binary FROM scope()
             })
          }) GROUP BY Binary

      // Get some tempfiles to work with.
      LET Config <= tempfile()
      LET Destination <= tempfile()

      // Choose the right target binary depending on the target OS
      LET tool_name = SELECT * FROM switch(
       a={ SELECT "VelociraptorWindows" AS Type FROM scope() WHERE OS = "Windows"},
       b={ SELECT "VelociraptorWindows_x86" AS Type FROM scope() WHERE OS = "Windows_x86"},
       c={ SELECT "VelociraptorLinux" AS Type FROM scope() WHERE OS = "Linux"},
       d={ SELECT "VelociraptorDarwin" AS Type FROM scope() WHERE OS = "MacOS"},
       e={ SELECT "" AS Type FROM scope()
           WHERE NOT log(message="Unknown target type " + OS) }
      )

      // Repack this binary.
      LET target_binary <= SELECT FullPath, Name
         FROM Artifact.Generic.Utils.FetchBinary(
            ToolName=tool_name[0].Type, SleepDuration="0",
            ToolInfo=inventory_get(tool=tool_name[0].Type))
         WHERE log(message="Target binary " + Name + " is at " + FullPath)

      // This is what we will call it.
      LET CollectorName <= format(
          format='Collector_%v',
          args=[target_binary[0].Name, ])

      // Create a zip file with the binaries in it.
      LET _ <= SELECT * FROM collect(artifacts="PackageToolsArtifact",
         output=Payload, args=dict(PackageToolsArtifact=dict(Binaries=Binaries.Binary)),
         artifact_definitions=PackageToolsArtifact)

      LET CollectionArtifact <= SELECT Value FROM switch(
        a = { SELECT StandardCollection AS Value FROM scope() WHERE target = "ZIP" },
        b = { SELECT S3Collection + CloudCollection AS Value  FROM scope() WHERE target = "S3" },
        c = { SELECT GCSCollection + CloudCollection AS Value  FROM scope() WHERE target = "GCS" },
        d = { SELECT SFTPCollection + CloudCollection AS Value  FROM scope() WHERE target = "SFTP" },
        e = { SELECT "" AS Value  FROM scope() WHERE log(message="Unknown collection type " + target) }
      )

      LET definitions <= SELECT * FROM chain(
      a = { SELECT name, description, tools, parameters, sources, reports
            FROM artifact_definitions(names=artifacts)
            WHERE name =~ "^(Custom|Packs)\\." AND
              log(message="Adding artifact_definition for " + name) },

      -- Support custom templates
      a2 = { SELECT name, description, tools, parameters, sources, reports
            FROM artifact_definitions(names=template)
            WHERE name =~ "^(Custom|Packs)\\." AND
              log(message="Adding artifact_definition for template " + name) },

      // Create the definition of the Collector artifact.
      b = { SELECT "Collector" AS name, (
                    dict(name="Artifacts",
                         default=serialize(format='json', item=artifacts),
                         type="json_array"),
                    dict(name="Parameters",
                         default=serialize(format='json', item=parameters),
                         type="json"),
                    dict(name="Template", default=template),
                    dict(name="Password", default=Password),
                    dict(name="Level", default=opt_level, type="int"),
                    dict(name="Format", default=opt_format),
                    dict(name="OutputPrefix", default=opt_output_directory),
                    dict(name="target_args",
                         default=serialize(format='json', item=target_args),
                         type="json"),
                ) AS parameters,
                (
                  dict(query=CollectionArtifact[0].Value),
                ) AS sources
            FROM scope() },
      c = { SELECT "Generic.Utils.FetchBinary" AS name,
            (
               dict(name="SleepDuration", type="int", default="0"),
               dict(name="ToolName"),
               dict(name="ToolInfo"),
               dict(name="IsExecutable", type="bool"),
            ) AS parameters,
            (
               dict(query=FetchBinaryOverride),
            ) AS sources FROM scope()  }
      )

      LET optional_cmdline = SELECT * FROM chain(
        a={ SELECT "-v" AS Opt FROM scope() WHERE opt_verbose},
        b={ SELECT "--nobanner" AS Opt FROM scope() WHERE NOT opt_banner},
        c={ SELECT "--require_admin" AS Opt FROM scope() WHERE opt_admin},
        d={ SELECT "--prompt" AS Opt FROM scope() WHERE opt_prompt},
        e={ SELECT "--tempdir" AS Opt FROM scope() WHERE opt_tempdir},
        f={ SELECT opt_tempdir AS Opt FROM scope() WHERE opt_tempdir}
      )

      // Build the autoexec config file depending on the user's
      // collection type choices.
      LET autoexec <= dict(autoexec=dict(
          argv=("artifacts", "collect", "Collector",
                "--logfile", CollectorName + ".log") + optional_cmdline.Opt,
          artifact_definitions=definitions)
      )

      LET me <= SELECT Exe FROM info()

      // Copy the configuration to a temp file and shell out to our
      // binary to repack it.
      LET repack_step = SELECT upload(
           file=Destination,
           accessor="file",
           name=CollectorName) AS Binary,
           timestamp(epoch=now()) As CreationTime
      FROM execve(argv=[
        me[0].Exe, "config", "repack",
        "--exe", target_binary[0].FullPath,
        "--append", Payload,
        copy(dest=Config,
             accessor='data',
             filename=serialize(format='json', item=autoexec)),
        Destination ], length=1000000)
      WHERE log(message="Creating config on " + Config) AND log(message=Stderr)

      // Only actually run stuff if everything looks right.
      SELECT * FROM if(condition=autoexec AND target_binary AND me[0].Exe,
         then=repack_step)
